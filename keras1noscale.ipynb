{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 138499\n",
      "Number of edges: 1091955\n"
     ]
    }
   ],
   "source": [
    "G=nx.read_edgelist('datasets/edgelist.txt', delimiter=',',create_using=nx.Graph(),nodetype=int)\n",
    "nodes=list(G.nodes())\n",
    "n=G.number_of_nodes()\n",
    "m=G.number_of_edges()\n",
    "\n",
    "print('Number of nodes:', n)\n",
    "\n",
    "print('Number of edges:', m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts=dict()\n",
    "with open('datasets/abstracts.txt', 'r',encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        node,abstract=line.split('|--|')\n",
    "        abstracts[int(node)]=abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_x_train(number_of_edges,list_of_features,mypath):\n",
    "    mypath=mypath\n",
    "    number_of_features=len(list_of_features)\n",
    "    print(\"number_of_edges:\",number_of_edges)\n",
    "    #mul by 2 for the training matrix\n",
    "    x=np.zeros((2*number_of_edges,number_of_features))\n",
    "    for idx,feature in enumerate(list_of_features):\n",
    "        print(\"loading column {} with feature {}\".format(idx,feature))\n",
    "        x[:,idx]=np.genfromtxt(mypath+feature,delimiter=',')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_x_test(number_of_edges,list_of_features,mypath):\n",
    "    mypath=mypath\n",
    "    number_of_features=len(list_of_features)\n",
    "    print(\"number_of_edges:\",number_of_edges)\n",
    "    #mul by 1 for the training matrix\n",
    "    x=np.zeros((number_of_edges,number_of_features))\n",
    "    for idx,feature in enumerate(list_of_features):\n",
    "        print(\"loading column {} with feature {}\".format(idx,feature))\n",
    "        x[:,idx]=np.genfromtxt(mypath+feature,delimiter=',')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile,join\n",
    "def get_feature_files(mypath):\n",
    "    mypath=mypath\n",
    "    features_to_include=[feature for feature in listdir(mypath)]\n",
    "    print(\"included features are: \",features_to_include)\n",
    "    return features_to_include\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "included features are:  ['adamic_adar_index.csv', 'common_elements_in_abstracts.csv', 'difference_of_abstracts_len.csv', 'jaccard_coef.csv', 'preferential_attachment.csv', 'resource_allocation_index.csv', 'sum_of_abstracts_len.csv']\n",
      "number_of_edges: 1091955\n",
      "loading column 0 with feature adamic_adar_index.csv\n",
      "loading column 1 with feature common_elements_in_abstracts.csv\n",
      "loading column 2 with feature difference_of_abstracts_len.csv\n",
      "loading column 3 with feature jaccard_coef.csv\n",
      "loading column 4 with feature preferential_attachment.csv\n",
      "loading column 5 with feature resource_allocation_index.csv\n",
      "loading column 6 with feature sum_of_abstracts_len.csv\n"
     ]
    }
   ],
   "source": [
    "mypath='datasets/features_train/'\n",
    "X_train=initialize_x_train(m,get_feature_files(mypath),mypath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=np.genfromtxt('datasets/Y_train.csv',delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_m,y_train_m=shuffle(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_pairs = list()\n",
    "with open('datasets/test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        #the raw split form is ['12223', '345332\\n']\n",
    "        #we need to make them integers\n",
    "        #use the function int()\n",
    "        node_pairs.append((int(t[0]), int(t[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "included features are:  ['adamic_adar_index_test.csv', 'common_elements_in_abstracts_test.csv', 'difference_of_abstracts_test.csv', 'jaccard_coefficient_test.csv', 'preferential_attachment_test.csv', 'resource_allocation_index_test.csv', 'sum_of_abstracts_len_test.csv']\n",
      "number_of_edges: 106692\n",
      "loading column 0 with feature adamic_adar_index_test.csv\n",
      "loading column 1 with feature common_elements_in_abstracts_test.csv\n",
      "loading column 2 with feature difference_of_abstracts_test.csv\n",
      "loading column 3 with feature jaccard_coefficient_test.csv\n",
      "loading column 4 with feature preferential_attachment_test.csv\n",
      "loading column 5 with feature resource_allocation_index_test.csv\n",
      "loading column 6 with feature sum_of_abstracts_len_test.csv\n"
     ]
    }
   ],
   "source": [
    "mypath='datasets/features_test/'\n",
    "X_test=initialize_x_test(len(node_pairs),get_feature_files(mypath),mypath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models,layers\n",
    "\n",
    "model=models.Sequential()\n",
    "model.add(layers.Dense(32,activation='relu',input_shape=(7,)))\n",
    "model.add(layers.Dense(32,activation='relu'))\n",
    "model.add(layers.Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4266/4266 [==============================] - 16s 3ms/step - loss: 4.4886 - accuracy: 0.6301\n",
      "Epoch 2/5\n",
      "4266/4266 [==============================] - 15s 3ms/step - loss: 2.6769 - accuracy: 0.6551\n",
      "Epoch 3/5\n",
      "4266/4266 [==============================] - 14s 3ms/step - loss: 2.0038 - accuracy: 0.6671\n",
      "Epoch 4/5\n",
      "4266/4266 [==============================] - 14s 3ms/step - loss: 1.5881 - accuracy: 0.6738\n",
      "Epoch 5/5\n",
      "4266/4266 [==============================] - 14s 3ms/step - loss: 1.2160 - accuracy: 0.6820\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X_train,y_train,epochs=5,batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.09372866e-01 1.09039094e-04 8.42505813e-01 4.29038167e-01\n",
      " 2.31334865e-02 6.76827610e-01 4.02840257e-01 4.35677916e-01\n",
      " 3.20359230e-01 8.95888805e-01 4.05323595e-01 5.93736649e-01\n",
      " 7.79803991e-01 2.65132725e-01 4.59791809e-01 4.54013556e-01\n",
      " 5.31662881e-01 4.68701720e-01 2.13773966e-01 3.41799855e-02]\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict(X_test)\n",
    "print(y_pred[:20,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "predictions=list(zip(range(len(y_pred)),y_pred[:,0]))\n",
    "with open(\"submission_keras1noscale.csv\", 'w') as outfile:\n",
    "    csv_out=csv.writer(outfile)\n",
    "    csv_out.writerow(['id','predicted'])\n",
    "\n",
    "    for row in predictions: \n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "bert_preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
    "bert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input=tf.keras.layers.Input(shape=(),dtype=tf.string,name='text')\n",
    "preprocessed_text=bert_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys       : ['input_type_ids', 'input_mask', 'input_word_ids']\n",
      "Shape      : (1, 128)\n",
      "Word Ids   : [ 101 2023 2003 1037 2204 3185  102    0    0    0    0    0]\n",
      "Input Mask : [1 1 1 1 1 1 1 0 0 0 0 0]\n",
      "Type Ids   : [0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "text_test=['this is a good movie']\n",
    "text_preprocessed=bert_preprocess(text_test)\n",
    "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
    "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
    "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
    "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
    "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded BERT: <tensorflow_hub.keras_layer.KerasLayer object at 0x00000101EB7CC670>\n",
      "Pooled Outputs Shape:(1, 768)\n",
      "Pooled Outputs Values:[-0.8787369  -0.28986096  0.21888238  0.6509246  -0.05978949 -0.08652615\n",
      "  0.8549417   0.19872479 -0.0367921  -0.9998593  -0.00700509  0.4182273 ]\n",
      "Sequence Outputs Shape:(1, 128, 768)\n",
      "Sequence Outputs Values:[[ 0.05524094  0.09279849  0.18011808 ... -0.20717065  0.25018063\n",
      "   0.10174131]\n",
      " [-0.5319642  -0.415677    0.28602126 ... -1.082092    1.0272561\n",
      "   0.15845591]\n",
      " [ 0.06295532 -0.500933    0.52670294 ... -0.5936099   0.5590759\n",
      "   0.6334749 ]\n",
      " ...\n",
      " [ 0.02546454 -0.42473173  0.4675644  ...  0.20326579  0.40612566\n",
      "  -0.18301186]\n",
      " [ 0.07985108 -0.46011198  0.4119412  ...  0.16304101  0.42579618\n",
      "  -0.37503162]\n",
      " [ 0.08701746 -0.43609232  0.4363102  ...  0.20113279  0.4420772\n",
      "  -0.23319733]]\n"
     ]
    }
   ],
   "source": [
    "bert_model=tf_hub.KerasLayer(bert_encoder)\n",
    "bert_results=bert_model(text_preprocessed)\n",
    "print(f'Loaded BERT: {bert_encoder}')\n",
    "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
    "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
    "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
    "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "    text_input=tf.keras.layers.Input(shape=(),dtype=tf.string,name='text')#our abstract\n",
    "    preprocessing_layer=tf_hub.KerasLayer(bert_preprocess,name='preprocessing')\n",
    "    encoder_inputs=preprocessing_layer(text_input)\n",
    "    encoder=tf_hub.KerasLayer(bert_encoder,trainable=True,name='BERT_encoder')\n",
    "    outputs=encoder(encoder_inputs)\n",
    "    net=outputs['pooled_output']\n",
    "    net=tf.keras.layers.Dropout(0.1)(net)\n",
    "    net=tf.keras.layers.Dense(1,activation=None,name='classifier')(net)\n",
    "    return tf.keras.Model(text_input,net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:absl:hub.KerasLayer is trainable but has zero trainable weights.\n"
     ]
    }
   ],
   "source": [
    "classifier=build_classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.73604375]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "bert_raw_result=classifier(tf.constant(text_test))\n",
    "print(tf.sigmoid(bert_raw_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertModel   \n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231508/231508 [00:00<00:00, 652928.44B/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], train_texts))\n",
    "test_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], test_texts))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "da22d4198b655ef0960ccd2ab85141aaa05c0c035eb04e9505cdd8240da58ecf"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
